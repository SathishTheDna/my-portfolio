"use strict";(self.webpackChunkmy_portfolio=self.webpackChunkmy_portfolio||[]).push([[580],{3995:(e,t,i)=>{i.r(t),i.d(t,{default:()=>ee});var a=i(5043),n=i(6971);i.p;const o=i.p+"static/media/ar-digital-twin-2.f514a45e6ad30bbfa8b6.png";const s={ProjectId:"ar-digital-twin",ProjectHeader:{title:"AR Digital Twin",timeline:"Sep 2019 - Nov 2019",tags:"Unity, AR, IoT"},ProjectMedia:[{id:1,type:"image",title:"AR Digital Twin",img:i(1117)},{id:2,type:"video",title:"AR Digital Twin",url:"https://www.youtube.com/watch?v=L2m9lddwmRQ"},{id:3,type:"image",title:"AR Digital Twin",img:o}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","ARCore","Firebase","NodeMCU","Arduino IDE"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"Utilized IoT and AR to create a digital twin, demonstrating the power of AR in visualizing real-time industrial data.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"During our IoT course at Amrita University, Coimbatore, India, our team embarked on an innovative project that merged IoT technology with Augmented Reality (AR) to visualize real-time data from industrial equipment. Our goal was to showcase the concept of Digital Twin in AR, providing valuable insights into equipment performance and maintenance."},{id:2,details:"We employed a NodeMCU to interface with various sensors, including an LM35 analog temperature sensor, a DHT11 digital temperature and humidity sensor, and an HC-SR04 ultrasonic sensor. These sensors simulated data typically received from industrial equipment, enabling us to create a comprehensive visualization of the operational environment."},{id:3,details:"By collecting and transmitting data from these sensors to a Firebase Real-time database, we laid the groundwork for our AR visualization. I then developed a Unity app to receive and present this data in AR, leveraging free 3D assets to craft a visually pleasing interface."},{id:4,details:"This project illuminated the potential of Digital Twin and AR visualization in optimizing industrial operations. Moving forward, we planned to enhance the project by integrating predictive maintenance algorithms and remote monitoring capabilities, further solidifying its value proposition in industrial contexts."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[]}};var r=i(3418);i.p;const l=i.p+"static/media/pongal-3.cde5e9b0da87d041ce2d.png",d=(i.p,i.p,{ProjectId:"pongal-greetings",ProjectHeader:{title:"AR Pongal Greetings",timeline:"Dec 2017 - Jan 2018",tags:"Unity, AR"},ProjectMedia:[{id:1,type:"image",title:"AR Pongal Greetings",img:r},{id:2,type:"video",title:"AR Pongal Greetings",url:"https://www.youtube.com/watch?v=2tIktbYM9Nc"},{id:3,type:"image",title:"AR Pongal Greetings",img:l}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","Vuforia"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"Immersive AR Greeting Card for a renowned festival in South India.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"Developed this app independently to showcase the cultural richness of Pongal, a renowned festival in South India, and pay tribute to the resilience of Jallikattu protesters through augmented reality (AR). This AR application offers a marker-based experience where users can scan any Indian currency note to reveal a vibrant greeting card."},{id:2,details:"Utilizing free assets sourced online, I crafted this beautiful app, deploying it on the Google Play Store in 2018. The primary aim was to raise awareness about Augmented Reality during a time when many people were unfamiliar with its capabilities and potential."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[]}}),c=(i.p,i.p+"static/media/drone-2.98bdeb0aa46092a59751.png");const p={ProjectId:"emergency-search-rescue-drone",ProjectHeader:{title:"Emergency Search and Rescue Drone",timeline:"2018 - 2019",tags:"IoT"},ProjectMedia:[{id:1,type:"image",title:"ESR Drone",img:i(5635)},{id:2,type:"video",title:"ESR Drone",url:"https://www.youtube.com/watch?v=zR30gRxc1-o?start=125"},{id:3,type:"image",title:"ESR Drone",img:c}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Python","Raspberry Pi","PiHawk"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"A hexacopter for efficient search and rescue operations in disaster-struck areas.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"I spearheaded the Emergency Search and Rescue (ESR) Drone project during my tenure at Amrita University, Coimbatore, India, as part of the Cisco ThingQbator Program Cohort 2. This project aimed to provide timely aid during floods, a common natural calamity in the southern regions of India. The program's mission was to empower young innovators to transform ideas into prototypes and businesses."},{id:2,details:"Forming a team comprising two software engineering students, one aerospace engineering student, and myself, we chose to construct a Hexacopter equipped with a Raspberry Pi, RGB camera, and Video Transmitter. This choice was made for its enhanced redundancy, stability, payload capacity, maneuverability, and endurance, all of which are critical for effective search and rescue operations in challenging environments like flood zones."},{id:3,details:"The hexacopter we built served as a proof of concept for our project. Our goal was to deploy surveillance drones to explore inaccessible areas in flood zones, such as small rooftops, tree-covered regions, and buildings with trapped victims."},{id:4,details:"Our future plans included embedding day-night vision cameras, thermal cameras, and enhancing camera feed latency. Additionally, we aimed to implement computer vision algorithms for human detection and integrate victim location tagging into a common interface for first responders and government agencies. These advancements would facilitate quick assessment of flood zones, enabling authorities to plan efficient rescue operations."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[{id:1,name:"Certificate",url:"https://drive.google.com/drive/folders/1Tarfjzbklv95NnfM1i37UV1w71RnjO0a?usp=sharing"}]}},g=i.p+"static/media/glove-gesture-1.187085f5af3f353c5bd4.png";const m={ProjectId:"gesture-recognition-glove",ProjectHeader:{title:"Real-time Numerical Gesture Recognition using MPU 9250 motion sensor",timeline:"Aug 2019 - May 2020",tags:"Unity, IoT"},ProjectMedia:[{id:1,type:"image",title:"Gesture Glove",img:i(722)},{id:2,type:"image",title:"Gesture Glove",img:g},{id:3,type:"image",title:"Gesture Glove",img:i.p+"static/media/glove-gesture-3.b2a2993c35cd72a09a63.png"}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","Tensorflow Lite","NodeMCU","MPU 9250 Motion Sensor","Arduino IDE"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"Research Publication proposing a low-cost Data Glove for Gesture Recognition",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"Collaborated with a team of three students at Amrita University on real-time numerical gesture recognition project. In the realm of Human-Computer Interaction research, hand gesture recognition plays a pivotal role in advancing interactive systems. However, existing approaches predominantly rely on expensive camera-based or 3D depth sensors, which are sensitive to environmental changes. To address these limitations, our project proposes a cost-effective data glove for gesture recognition with a main focus for numerical gesture recognition."},{id:2,details:"We conducted a extensive research on existing solutions and evaluating the performance of different machine learning and neural network models. After thorough experimentation and analysis, I spearheaded the selection and training of a neural network model optimized for numerical gesture recognition. This involved fine-tuning the model parameters and validating its accuracy and efficiency. Subsequently, I converted the trained model to tensorflow lite flat buffers for seamless integration with our Unity application, leveraging the Tensorflow Lite C SDK for deployment. Our aim was to ensure that the system could perform real-time inference with minimal latency, a critical requirement for interactive applications."},{id:3,details:"To showcase the practical application of our solution, we developed a small car-lane changing game within Unity. This game served as a real-time demonstration of the glove's responsiveness and accuracy in recognizing numerical gestures, providing tangible evidence of its effectiveness. Additionally, we presented our findings and insights in a conference paper at the International Conference on Intelligent Computing, Information, and Control Systems. This dissemination of our research contributed to the broader academic and research community, highlighting the potential of our approach in advancing human-computer interaction technologies."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[{id:1,name:"Research Publication",url:"https://link.springer.com/chapter/10.1007/978-981-15-8443-5_4#:~:text=In%20this%20paper%2C%20an%20effective,hand%2C%20to%20predict%20numerical%20gestures."},{id:1,name:"Certificate and Documents",url:"https://drive.google.com/drive/folders/1HVtepCSkabNUFUkSdx7smFGBIIoIpSMe?usp=sharing"}]}};var h=i(9281);i.p,i.p;const u={ProjectId:"hull-cleaning-robot-simulator",ProjectHeader:{title:"Ship Hull Cleaning Robot Simulator",timeline:"July 2023 - Aug 2023",tags:"Unity"},ProjectMedia:[{id:1,type:"image",title:"Hull Cleaner",img:h}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","Cinemachine"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"Simulator to showcase the underwater cleaning efficiency of a hull cleaning robot",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"During my tenure at Mount Visual in Bergen, the Ship Hull Cleaning Robot Simulator was created for EcoSubsea AS. Independently crafted, this Mac OS application showcases the underwater effectiveness of the cleaning robot."},{id:2,details:"Featuring various ships, the simulator allows users to test the robot's efficiency in cleaning different hulls. The Inverse Kinematics (IK) for the robot ensures realistic movements during cleaning operations."},{id:3,details:"Visualizing the cleaning process, the simulator utilizes the Paint in 3D package sourced from the Unity Asset Store to depict the effect of the robot cleaning the hull."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[]}};const f={ProjectId:"itrollheimen",ProjectHeader:{title:"iTrollheimen AR",timeline:"April 2022 - July 2022",tags:"Unity, AR"},ProjectMedia:[{id:1,type:"image",title:"iTrollheimen",img:i(5613)},{id:2,type:"image",title:"iTrollheimen",img:i.p+"static/media/itrollheimen-2.1f5937501f631da8aea8.png"},{id:3,type:"image",title:"iTrollheimen",img:i.p+"static/media/itrollheimen-3.26321602d6010e0e10d9.png"}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","ARFoundation","Mapbox"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"AR adventures with trolls as storytellers",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"iTrollheimen is a whimsical location-based augmented reality (AR) experience, crafted during my tenure at BreachVR in Trondheim, to ignite the curiosity and imagination of children and young adults about nature and trolls in Trollheimen. Through the enchanting fusion of technology and storytelling, this innovative app invites users to embark on a GPS-position based adventure in nature, uncovering the mischief of trolls in Trollheimen."},{id:2,details:"Developed in collaboration with the team at Home of the Trolls, iTrollheimen offers a narrated journey featuring beloved characters Boll and B\xf8llu. Using Unity Visual Script Graphs, I scripted captivating stories that unfold as users explore their surroundings. Additionally, Mapbox integration provides detailed maps for navigation, enhancing the immersive experience."},{id:3,details:"Children are encouraged to explore their surroundings, discovering hidden secrets and learning from the trolls they encounter along the way. iTrollheimen offers a magical gateway to the wonders of nature, inspiring young adventurers to connect with their surroundings and embark on unforgettable journeys of discovery."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[{id:1,name:"Project Website",url:"https://breachvr.com/project/itrollheimen/"}]}};const v={ProjectId:"magipaint-ar",ProjectHeader:{title:"MagiPaint AR",timeline:"Feb 2019 - May 2019",tags:"Unity, AR"},ProjectMedia:[{id:1,type:"image",title:"MagiPaint AR",img:i.p+"static/media/magipaint-1.4a121a33457d88fa30fc.png"},{id:2,type:"image",title:"MagiPaint AR",img:i(546)},{id:3,type:"image",title:"MagiPaint AR",img:i.p+"static/media/magipaint-3.1ba10a17bc6ece234fdf.png"}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","Android Studio","Vuforia"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"Create, Save, and Share 3D drawings in AR.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"Developed while working as a part-time developer with Ludenso, this AR Painting App brings a new dimension to creative expression on Android mobile phones. Published on the Google Play Store, this app leverages Vuforia for an immersive augmented reality (AR) experience."},{id:2,details:"The app features a main base marker from Ludenso, serving as the canvas for users to draw on. A cube marker with six faces acts as the interactive menu, with a virtual brush emerging from it to pinpoint exact locations in the real world where users want to draw."},{id:3,details:"Key features:"},{id:4,details:"3D Drawing: Users draw in 3D space with respect to the main base marker. Lines and curves comprise datapoints, representing the position of each point in 3D space, connected through line renderers."},{id:5,details:"Android Integration: Developed an Android plugin using Android Studio to save, load, and share AR drawings in a custom file format."},{id:6,details:"Immersive Experience: Designed to be used with Ludenso\u2019s MagiMask AR Headset, where users insert their mobile phones into the headset. The real world is viewed through the phone's camera feed with virtual content overlaid on top of the marker for an immersive AR painting experience."},{id:7,details:"This app showcases the potential of AR technology in creative applications, providing users with a unique and engaging way to visualize their artistic ideas in the real world."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[{id:1,name:"Google PlayStore Link",url:"https://play.google.com/store/apps/details?id=com.techydna.magipaintar"}]}},y=i.p+"static/media/stack-1.173654bfa8bd99f1ff0a.png";i.p;var b=i(5537);const w=i.p+"static/media/stack-4.2bfb40051489cb3fc6d8.png",j=(i.p,i.p,{ProjectId:"magistack-ar",ProjectHeader:{title:"MagiStack AR",timeline:"Feb 2018 - Jan 2019",tags:"Unity, AR"},ProjectMedia:[{id:1,type:"image",title:"MagiStack AR",img:y},{id:2,type:"image",title:"MagiStack AR",img:b},{id:3,type:"image",title:"MagiStack AR",img:w}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","ARFoundation","ARCore","Vuforia","Manomotion Hand Tracking Technology"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"AR adaptation of the popular Stack game.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"Embark on an adventure with my AR Stack game, offering a fresh take on the classic Stack game. Stack blocks as high as possible in augmented reality, immersing yourself in simple yet stunning graphics designed for an unforgettable gaming experience. Compete against friends and yourself for the highest score, all within the realm of AR."},{id:2,details:"Choose from three captivating themes - Snow, Desert, or Water landscapes - each providing a unique backdrop for your stacking journey. Explore diverse stack block textures, while experiencing tactile haptic feedback and delightful sounds, enhancing the game's immersive nature."},{id:3,details:"Initially released as version 1, the game utilized Ludenso's proprietary markers with Vuforia, offering a marker-based experience. Later, I upgraded the game with ARCore for markerless tracking, enabling users to place the game on any surface, further enhancing the immersive experience."},{id:4,details:"In version 2, I collaborated with Manomotion to integrate hand gesture recognition capabilities, elevating the interactivity of the game. Manomotion used this app to showcase their mobile hand tracking SDK\u2019s capabilities in AWE and CES 2019."},{id:5,details:"Praised for its addictive gameplay, diverse themes, and immersive AR experience, the app garnered overwhelmingly positive reviews, solidifying its place as a must-try AR game."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[{id:1,name:"Google PlayStore Link",url:"https://play.google.com/store/apps/details?id=com.techydna.magistack"},{id:2,name:"AWE 2019 Showcase",url:"https://www.youtube.com/watch?v=dKYBUwtnUVY"},{id:3,name:"CES 2019 Showcase",url:"https://www.youtube.com/watch?v=xZeq9I6Cw_k"}]}});const x={ProjectId:"manomotion-addon-plugins",ProjectHeader:{title:"Manomotion SDK add-on plugins",timeline:"May 2019 - Sep 2019",tags:"Unity, AR"},ProjectMedia:[{id:1,type:"image",title:"Manomotion Add-on Plugins",img:i(254)}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","Manomotion SDK","ARFoundation","Wikitude","Vuforia"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"Developed Manomotion add-on plugins for ARFoundation, Wikitude, and Vuforia SDKs.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"During my internship at Manomotion, I contributed to the development of Vuforia, Wikitude, and ARFoundation add-on plugins for the Manomotion SDK. The Manomotion SDK enables hand tracking and gesture recognition in AR and VR applications, addressing the challenge of accurately detecting and interpreting hand movements and gestures in real-time. By enhancing user interaction and immersion in AR and VR experiences, the Manomotion SDK plays a crucial role in advancing the capabilities of interactive digital environments."},{id:2,details:"My contribution to the development of add-on plugins aimed to extend the functionality of the Manomotion SDK, enabling seamless integration with popular AR platforms and empowering developers to create more immersive and interactive AR applications. I collaborated with the community manager at Manomotion aimed to enhance hand tracking and gesture recognition capabilities in AR and VR applications, facilitating smoother development workflows and accelerating the adoption of advanced interaction features in AR experiences."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[]}};i(3953);const P=i.p+"static/media/manomotion-internship-2.ccbe9989c33bf2ce498d.png",k={ProjectId:"manomotion-sdk-showcase",ProjectHeader:{title:"Manomotion SDK Showcase",timeline:"Aug 2019 - Sep 2019",tags:"Unity, AR"},ProjectMedia:[{id:1,type:"image",title:"Manomotion SDK Showcase",img:i.p+"static/media/manomotion-internship-3.a93c6a9d5f33022d4b9f.png"},{id:2,type:"video",title:"Manomotion SDK Showcase",url:"https://www.youtube.com/watch?v=xEA3Lun_tRk"},{id:3,type:"image",title:"Manomotion SDK Showcase",img:P}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","Manomotion SDK"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"Developed an interactive app highlighting Manomotion SDK's performance and capabilities.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"During my internship at Manomotion, I developed an app to demonstrate the capabilities of the Manomotion SDK. This app showcased a Number Keypad and Grid with movable components, utilizing Manomotion's Click Gesture for keypad interaction and Click and Drag gesture for component manipulation within the grid. The app also displayed the frames per second (FPS) and processing time taken by the Manomotion SDK for each frame, providing insight into its performance."},{id:2,details:"The Manomotion SDK enables hand tracking and gesture recognition in AR and VR applications, addressing the challenge of accurately detecting and interpreting hand movements and gestures in real-time. By enhancing user interaction and immersion in AR and VR experiences, the Manomotion SDK plays a crucial role in advancing the capabilities of interactive digital environments."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[{id:1,name:"Internship Completion Certificate",url:"https://drive.google.com/drive/folders/1ZM54hHb0rbGqAy1QsZ7VIeVSdhI_5Ox-"}]}};const R={ProjectId:"multi-touch-supported-browsers",ProjectHeader:{title:"Multi-touch supported Browsers in Unity apps",timeline:"Oct 2023 - Nov 2023",tags:"Unity"},ProjectMedia:[{id:1,type:"image",title:"Multi-touch supported browsers",img:i(537)}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","Mapbox","Vuplex WebViews"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"Enhanced multi-touch interactions in Vuplex WebViews.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"During my tenure at Mount Visual in Bergen, I spearheaded the development of a demo application to elevate user experience in Unity apps using Vuplex WebViews. With a primary focus on enabling seamless interaction with browser applications like Google Maps and Mapbox on multi-touch screens, I crafted custom touchscript gestures to prioritize touch inputs, ensuring glitch-free functionality."},{id:2,details:"Furthermore, I seamlessly integrated these gestures with the Vuplex plugin, guaranteeing compatibility and smooth performance. This significant step laid a solid foundation for future developments at Mount Visual, promising cutting-edge user experiences with browsers in Unity applications."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[]}};const D={ProjectId:"multi-touch-product-showcase",ProjectHeader:{title:"Multi-touch supported Product Display App",timeline:"Feb 2023 - May 2023",tags:"Unity"},ProjectMedia:[{id:1,type:"image",title:"Multi-touch Product Showcase",img:i(3765)},{id:2,type:"image",title:"Multi-touch Product Showcase",img:i.p+"static/media/kongsberg-2.3d269e0eb722907a49ab.jpg"}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","TUIO","TouchScript"]}],ObjectivesHeading:"Overview",ObjectivesDetails:'Experience interactive product showcases on a 55" multi-touch display with object recognition support.',ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:'During my tenure at Mount Visual in Bergen, I developed a multi-touch supported app for Kongsberg Digital AS to showcase their products at conferences. Tailored specifically for a 55" multi-touch display with object recognition support, the app utilized a TUIO Client package and TouchScript to enable multi-touch Unity UI interactions.'},{id:2,details:"The display, known as the Scape Lab, is manufactured and sold by Interactive Scape in Germany. It offers a blend of flexibility and stability, making it ideal for various environments including laboratories, institutes, offices, and educational institutions. Its wheeled steel frame ensures easy mobility, while the 24/7 operation allows uninterrupted productivity. With multi-touch support and object recognition capabilities, the interactive experience is enhanced, allowing users to explore products with ease and interactivity."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[]}};var T=i(9978);const I=i.p+"static/media/salmon-2.ffee9e545d094d20f91b.jpg",S=i.p+"static/media/salmon-3.005187ce389cbf921e36.jpg",A=(i.p,i.p,i.p,i.p,{ProjectId:"salmon-farm-showcase",ProjectHeader:{title:"Salmon Farm Showcase App",timeline:"Oct 2022 - June 2023",tags:"Unity"},ProjectMedia:[{id:1,type:"image",title:"Salmon Farm Showcase",img:T},{id:2,type:"image",title:"Salmon Farm Showcase",img:S},{id:3,type:"image",title:"Salmon Farm Showcase",img:I}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","Cinemachine","TouchScript"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"Interactive Visualization of a Salmon Farm.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"During my tenure at Mount Visual in Bergen, I developed the Salmon Farm Showcase App for Sterner AS. This interactive application highlights Sterner's Salmon Farming unit, showcasing the entire facility and its important equipment."},{id:2,details:"The main view of the app presents an overview of the entire unit, with location pins indicating important rooms within the building. Users can click on these pins to navigate to specific locations within the unit, facilitated by Cinemachine for smooth transitions."},{id:3,details:"To enhance realism and immersion, compute shaders were utilized to implement salmon boids and particle flocking effects, adding dynamic movement and visual interest to the simulation."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[]}});const U={ProjectId:"storybits",ProjectHeader:{title:"Storybits AR",timeline:"Jan 2022 - Mar 2022",tags:"Unity, AR"},ProjectMedia:[{id:1,type:"image",title:"Storybits AR",img:i(2433)}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","ARFoundation","Mapbox"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"A location-based AR story telling app.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"During my tenure at Breach VR in Trondheim, I contributed to StoryBits AR, an innovative location-based augmented reality storytelling application that brings narratives to life in the real world using Mapbox. Seamlessly blending the digital and physical realms, users embark on a captivating journey as they explore the story in the map. Utilizing AR and 2D puzzles, participants unlock hidden stories, each waiting to be discovered at specific locations."},{id:2,details:"With StoryBits AR, users are not just passive observers but active participants in the narrative. As they navigate through the story in the map, they engage with interactive elements, solve puzzles, and uncover the rich tapestry of stories waiting to be revealed."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[]}};const H={ProjectId:"tuio-client-package",ProjectHeader:{title:"Unity Tuio Client",timeline:"Mar 2023 - Dec 2023",tags:"Unity"},ProjectMedia:[{id:1,type:"image",title:"Tuio Client Package",img:i(9697)}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","TUIO","TouchScript","C++"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"TUIO Input processing package for multi-touch supported apps.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"Developed during my tenure at Mount Visual in Bergen, the Unity TUIO Touch Input Package is a versatile solution designed to process multi-touch screen input received via the TUIO protocol. Many large screen displays supporting multi-touch utilize the TUIO protocol to transmit touch data, which isn't inherently compatible with Unity UI and Input System. Thus, there arose a need to bridge this gap by processing the touch data and integrating it with the Unity Input System."},{id:2,details:"This package seamlessly processes TUIO touch data and feeds it into the Unity InputSystem, enabling smooth integration with Unity applications. Additionally, it integrates with TouchScript to facilitate multi-touch UI inputs and gesture recognition, enhancing the versatility of the solution."},{id:3,details:"This Unity TUIO Client Package serves as a comprehensive solution for any multi-touch application, offering compatibility and functionality across various platforms and environments."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[]}};const M={ProjectId:"vr-architecture-visualization",ProjectHeader:{title:"VR Architecture Visualization App",timeline:"May 2018 - July 2018",tags:"Unity, VR"},ProjectMedia:[{id:1,type:"image",title:"VR Arch Visualization",img:i(5944)},{id:2,type:"image",title:"VR Arch Visualization",img:i.p+"static/media/vr-architecture-2.127cb233fb73a1a69791.png"}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","Lenovo Mirage Solo VR Headset"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"Explore and personalize your virtual living space seamlessly with this intuitive app.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"Step into the realm of virtual architecture with my VR Architecture Visualization App, developed during an internship at Focuz AR. Tailored for the Lenovo Mirage Solo VR headset, this app redefines how users interact with apartment designs."},{id:2,details:"Users are transported into a virtual apartment where they can navigate seamlessly and explore their space. They can unleash their creativity by adding furniture from a diverse gallery and customizing textures and colors to reflect their unique style and preferences."},{id:3,details:"Beyond personalization, this app fosters collaboration and communication in the design process. Whether seeking feedback from friends or collaborating with interior designers, users can easily share their designs and ideas, facilitating productive discussions and informed decisions."},{id:4,details:"Future development plans for this demo included the ability to save and share modifications, enhancing the collaborative and iterative design process."},{id:5,details:"From conceptualization to execution, my VR Architecture Visualization App empowers users to iterate and innovate, transforming their vision into reality within the virtual realm."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[{id:1,name:"Internship Completion Certificate",url:"https://drive.google.com/drive/folders/1zSK_ZAPVaOsX0KPjkdLixhvUz424JiMx"}]}};const V={ProjectId:"vr-dev-toolkit",ProjectHeader:{title:"VR Development Toolkit",timeline:"May 2020 - Dec 2020",tags:"Unity, VR"},ProjectMedia:[{id:1,type:"image",title:"VR Development Toolkit",img:i(5517)}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","Oculus Quest 1","HTC Vive","OpenVR"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"Contributed to Breach VR's multiplayer cross-platform application development toolkit.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"During my tenure at Breach VR, I significantly contributed to the enhancement and expansion of their VR Development Toolkit, a versatile multiplayer cross-platform solution catering to Oculus headsets, HTC Vive, and Windows machines."},{id:2,details:"Through refactoring and feature additions across different subsystems, the toolkit's functionality and usability were notably improved, enabling its seamless integration into various projects at Breach VR, including XR Presentation Rooms and VR Fire Training simulations."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[]}};var O=i(3023);i.p;const L={ProjectId:"vr-hand-keypad",ProjectHeader:{title:"VR Hand Keypad",timeline:"Apr 2020 - May 2020",tags:"Unity, VR"},ProjectMedia:[{id:1,type:"image",title:"VR Hand Keypad",img:O},{id:2,type:"video",title:"VR Hand Keypad",url:"https://www.youtube.com/watch?v=tIxtW-0DnGU"},{id:3,type:"image",title:"VR Hand Keypad",img:i.p+"static/media/vr-hand-keypad-3.ea31eb7338bc818fab54.png"}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","Oculus Quest 1","Tensorflow Lite"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"Revolutionizing virtual interaction with intuitive hand gestures.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"During my internship at Breach VR, I sought innovative approaches to enhance VR interaction systems, leading me to explore the potential of natural hand gestures for intuitive interaction."},{id:2,details:"Inspired by the expressiveness of human hands, I conceived the idea of the VR Hand Keyboard, envisioning the palm surface between finger joints as buttons in a virtual number keypad."},{id:3,details:"I meticulously identified key hand tracking data from the Oculus Hand Tracking system that could facilitate accurate gesture recognition for the VR Hand Keyboard and gathered the data for each key presses."},{id:4,details:"Using the collected data, I trained a neural network model to detect keypad presses without false positives, ensuring robust performance in real-world scenarios."},{id:5,details:"To integrate the trained model into a Unity app, I navigated the challenges of building a TensorFlow Lite C SDK for Unity, overcoming hurdles due to the lack of documentation. I converted the Tensorflow Lite saved model to Tensorflow Lite Flat Buffers using Tensorflow Lite Converter."},{id:6,details:"With the trained model successfully integrated, I deployed the VR Hand Keyboard in the Unity app, enabling seamless real-time gesture recognition for intuitive interaction in virtual environments."},{id:7,details:"The VR Hand Keyboard project offers a multifaceted solution with diverse applications. It enables users to input text seamlessly in virtual environments, enhances gaming experiences through intuitive gesture controls, fosters interactive learning tools for educational purposes, and provides accessible input methods for individuals with physical disabilities. Additionally, it facilitates collaboration and communication in virtual meetings or conferences, underscoring its versatility and potential impact across various domains."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[]}};var E=i(2201);i.p,i.p;const C={ProjectId:"vr-locomotion-system",ProjectHeader:{title:"Hands-only Locomotion System for Oculus Quest",timeline:"Feb 2019 - May 2019",tags:"Unity, VR"},ProjectMedia:[{id:1,type:"image",title:"VR Locomotion",img:E},{id:2,type:"image",title:"VR Locomotion",img:i.p+"static/media/locomotion-4.71cb7051c9bebad1a074.png"},{id:3,type:"image",title:"VR Locomotion",img:i.p+"static/media/locomotion-5.a4dbc2253ca2e9af8d4f.png"}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","Oculus Quest 1","Tensorflow Lite"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"A comprehensive locomotion system with static and dynamic hand gestures as input.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"This project was part of my internship at Breach VR in Trondheim and also served as my bachelor's thesis in Computer Science Engineering at Amrita University. The primary aim was to create a natural and immersive locomotion system for VR using Oculus Quest\u2019s hand tracking technology, eliminating the need for traditional VR controllers."},{id:2,details:"Traditional VR controllers, while functional, reduce the naturalness and immersiveness of VR experiences. The Oculus Quest\u2019s hand tracking capabilities offer an opportunity to improve these aspects by using real hands as the primary input modality."},{id:3,details:"The project involved investigating various hand gestures and movements to determine the most intuitive methods for VR locomotion. Multiple iterations of the hand-based locomotion system were developed, incorporating user feedback to refine and enhance the user experience. Addressing the limitations of computer-vision based hand tracking was crucial to ensure reliability and responsiveness in different VR scenarios."},{id:4,details:"This work explored the potential of using real hands as the input modality for VR locomotion. Several hand gestures were proposed, with static gestures outperforming dynamic ones. For critical functions like locomotion, the system was designed to avoid false positive errors, ensuring precise and intentional movements in VR."},{id:5,details:"The continuous and non-continuous locomotion modules developed can be readily deployed in any VR application. The teleportation technique proved best for non-continuous locomotion, while static single-hand gestures were most effective for continuous locomotion in a sitting position. For standing positions, dynamic gestures for walking, jumping, flying, and swimming were implemented successfully, though dynamic gesture recognition faced challenges with false positives due to hand tracking noise in Oculus Quest 1."},{id:6,details:"Despite the challenges, the developed system demonstrated the feasibility of hand-based VR locomotion. With advancements in hand tracking technology, such systems will become more robust. The proposed gestures and locomotion methods are not limited to Oculus Quest but can be adapted for future VR headsets with hand tracking capabilities."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[{id:1,name:"Demo Videos",url:"https://www.youtube.com/playlist?list=PLukur2Itr38hjYYz4604G7Fd1-_hWbd7E"},{id:2,name:"Bachelor Degree Thesis Documents",url:"https://drive.google.com/drive/folders/1dcRdWIGYduhs0uA7kBf8RuTSOvqn_t8B?usp=sharing"},{id:3,name:"Internship Completion Certificate",url:"https://drive.google.com/drive/folders/138wzuj1raJX7pOgcXlUFha3XFGRLdiM8?usp=sharing"}]}};var z=i(2308);const F=i.p+"static/media/vr-ship-2.beca5f4f6c8261a979f3.png",N=(i.p,i.p,i.p,i.p,{ProjectId:"vr-ship-and-bridge-simulator",ProjectHeader:{title:"VR Ship and Bridge Simulator",timeline:"Nov 2022 - Feb 2023",tags:"Unity, VR"},ProjectMedia:[{id:1,type:"image",title:"VR Ship and Bridge Simulator",img:z},{id:2,type:"image",title:"VR Ship and Bridge Simulator",img:F},{id:3,type:"image",title:"VR Ship and Bridge Simulator",img:i.p+"static/media/vr-ship-7.e9caff98db89d1394073.png"}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","Oculus Quest 2"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"Simulator to train users about Safe Navigation practices.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"VR Ship and Bridge Simulator is an immersive training solution developed during my tenure at Mount Visual in Bergen, specifically for Sayr AS as a demonstration of cost-effective safety navigation practices. Recognizing the challenges and costs associated with traditional in-person ship navigation training, we proposed and executed a comprehensive VR simulator to train individuals in safe navigation practices."},{id:2,details:"Featuring animated full-body lipsynced avatars, this VR simulator offers users an interactive learning experience. I spearheaded the development independently, scripting the training modules using Unity Visual Script Graphs. Leveraging avatars sourced from another Mount Visual project, capable of responding to scripted modules, I ensured a dynamic and engaging training environment."},{id:3,details:"Designed as a hands-only VR experience, this simulator includes interactive components for navigating the ship. Each training module offers branching pathways based on user selections, guiding them through various safety practices. Users progress through the modules by completing steps aligned with safety protocols, providing an effective and efficient training solution for Sayr AS."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[]}});var B=i(621);const K=i.p+"static/media/xr-presentations-2.d3ac732b7a827fed4a53.png",G=(i.p,i.p,[{ProjectId:"xr-presentation-rooms",ProjectHeader:{title:"XR Presentation Rooms",timeline:"Jun 2020 - Dec 2021",tags:"Unity, VR"},ProjectMedia:[{id:1,type:"image",title:"XR Presentation Rooms",img:B},{id:2,type:"video",title:"XR Presentation Rooms",url:"https://www.youtube.com/watch?v=ZlqbBMvFOp8?start=872"},{id:3,type:"image",title:"XR Presentation Rooms",img:K}],ProjectInfo:{Technologies:[{title:"Tools & Technologies",techs:["Unity","Oculus Quest 1 and 2","HTC Vive","OpenVR","OpenXR","WebRTC","Photon Unity Networking"]}],ObjectivesHeading:"Overview",ObjectivesDetails:"Multi-player, cross-platform XR app for virtual meetings and collaboration.",ProjectDetailsHeading:"Project Details",ProjectDetails:[{id:1,details:"XR Presentation Rooms is a multi-player, cross-platform XR meeting room application developed during my time at Breach VR. Designed to facilitate collaboration, engagement, and interaction within shared virtual environments, the app offers a variety of presentation rooms for scheduling meetings, with easy-to-enter meeting codes for inviting participants."},{id:2,details:"Users can upload presentation materials such as images, videos, documents, and 3D models via a web dashboard before the meeting. During meetings, participants can utilize an HD presentation screen, spawn 3D models, and display images in a 3D plane. Persistent objects like sticky notes, 3D models, and images remain available for ongoing discussions in future sessions."},{id:3,details:"The app supports streaming from the virtual environment to any social platform and is compatible with standalone VR headsets, PC VR headsets like Oculus and HTC Vive, and Windows PCs."},{id:4,details:"Additionally, browser users can join meetings with video and audio feeds, ensuring seamless interaction between virtual and browser participants. The app also features a fully customizable avatar system optimized for both aesthetics and performance."},{id:5,details:"I played a pivotal role in both the development and maintenance of the XR Presentation Rooms app while working with Breach VR, collaborating with a team and independently as needed. My contributions include:"},{id:6,details:"1. Development and Maintenance: During the development phase, people at Breach VR were the testers. We had a company-wide weekly meeting for which I prepared the builds, gathered feedback after the meeting, fixed bugs, and prepared new builds with more features."},{id:7,details:"2. Avatar System: Researched third-party avatar systems and, for performance considerations, collaborated with a 3D artist to build a custom avatar system. This system includes diverse customization options, integrated lipsync with Oculus LipSync, and a gesture-based facial expression system (e.g., single thumbs-up for a happy expression and double thumbs-up for super happy)."},{id:8,details:"3. Productivity Tools: Developed essential tools such as persistent 3D models, sticky notes, an HD presentation screen, and a cloud file browser with integrated presentation capabilities."},{id:9,details:"4. Streaming and Access: Enabled streaming and easy access to VR meetings from any device with web browsers using the Ant Media WebRTC server and Unity WebRTC package."},{id:10,details:"5. Performance Optimization: Maintained a consistent 72 FPS in BreachLab by utilizing Unity Profiler, Memory Profiler, Frame Debugger, RenderDoc, and various code optimization techniques."},{id:11,details:"XR Presentation Rooms exemplifies innovation in virtual collaboration, offering robust tools and seamless integration to enhance productivity and engagement in a virtual setting."}],ExternalLinksHeading:"Useful Links",ExternalLinks:[{id:1,name:"Project Website",url:"https://breachvr.com/project/xr-presentations/"}]}},N,f,C,L,j,v,p,A,M,D,m,V,H,U,x,k,u,R,d,s]);var W=i(579);const q=(0,a.createContext)(),X=e=>{let{children:t}=e;const{projectId:i}=(0,n.g)(),o=(0,n.Zp)(),[s,r]=(0,a.useState)(G[0]);return(0,a.useEffect)((()=>{const e=G.find((e=>e.ProjectId.toString()===i));r(e)}),[i]),s?(0,W.jsx)(q.Provider,{value:{singleProjectData:s,setSingleProjectData:r},children:t}):(o("/"),null)},Q=q,J=()=>{const{singleProjectData:e}=(0,a.useContext)(Q);return(0,W.jsx)("div",{className:"grid grid-cols-1 sm:grid-cols-3 sm:gap-10 mt-12",children:e.ProjectMedia.map((e=>(0,W.jsx)("div",{className:"mb-10 sm:mb-0",children:"video"===e.type?(0,W.jsx)("iframe",{width:"100%",height:"100%",src:"https://www.youtube.com/embed/".concat(e.url.split("v=")[1]),title:e.title,allow:"accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture",className:"rounded-xl cursor-pointer shadow-lg sm:shadow-none",allowFullScreen:!0}):(0,W.jsx)("img",{src:e.img,className:"rounded-xl cursor-pointer shadow-lg sm:shadow-none",alt:e.title},e.id)},e.id)))})};var _=i(5200);const Y=()=>{const{singleProjectData:e}=(0,a.useContext)(Q);return(0,W.jsxs)("div",{children:[(0,W.jsx)("p",{className:"font-general-medium text-left text-3xl sm:text-4xl font-bold text-primary-dark dark:text-primary-light mt-14 sm:mt-20 mb-7",children:e.ProjectHeader.title}),(0,W.jsxs)("div",{className:"flex",children:[(0,W.jsxs)("div",{className:"flex items-center mr-10",children:[(0,W.jsx)(_.Ohp,{className:"text-lg text-ternary-dark dark:text-ternary-light"}),(0,W.jsx)("span",{className:"font-general-regular ml-2 leading-none text-primary-dark dark:text-primary-light",children:e.ProjectHeader.timeline})]}),(0,W.jsxs)("div",{className:"flex items-center",children:[(0,W.jsx)(_.cnX,{className:"text-lg text-ternary-dark dark:text-ternary-light"}),(0,W.jsx)("span",{className:"font-general-regular ml-2 leading-none text-primary-dark dark:text-primary-light",children:e.ProjectHeader.tags})]})]})]})},Z=()=>{var e,t;const{singleProjectData:i}=(0,a.useContext)(Q);return(0,W.jsxs)("div",{className:"block sm:flex gap-0 sm:gap-10 mt-14",children:[(0,W.jsxs)("div",{className:"w-full sm:w-1/3 text-left",children:[(0,W.jsxs)("div",{className:"mb-7",children:[(0,W.jsx)("p",{className:"font-general-regular text-2xl font-semibold text-ternary-dark dark:text-ternary-light mb-2",children:i.ProjectInfo.ObjectivesHeading}),(0,W.jsx)("p",{className:"font-general-regular text-primary-dark dark:text-ternary-light",children:i.ProjectInfo.ObjectivesDetails})]}),(0,W.jsxs)("div",{className:"mb-7",children:[(0,W.jsx)("p",{className:"font-general-regular text-2xl font-semibold text-ternary-dark dark:text-ternary-light mb-2",children:i.ProjectInfo.Technologies[0].title}),(0,W.jsx)("p",{className:"font-general-regular text-primary-dark dark:text-ternary-light",children:i.ProjectInfo.Technologies[0].techs.join(", ")})]}),0!==(null!==(e=null===(t=i.ProjectInfo.ExternalLinks)||void 0===t?void 0:t.length)&&void 0!==e?e:0)&&(0,W.jsxs)("div",{className:"mb-7",children:[(0,W.jsx)("p",{className:"font-general-regular text-2xl font-semibold text-ternary-dark dark:text-ternary-light mb-2",children:i.ProjectInfo.ExternalLinksHeading}),i.ProjectInfo.ExternalLinks.map((e=>(0,W.jsxs)("a",{href:e.url,target:"__blank",rel:"noopener noreferrer",className:"text-gray-400 hover:text-primary-dark dark:hover:text-primary-light shadow-sm duration-500 flex items-center mb-3",children:[(0,W.jsx)("span",{children:e.name}),(0,W.jsx)(_.HaR,{className:"text-lg lg:text-2xl ml-1"})]},e.id)))]})]}),(0,W.jsxs)("div",{className:"w-full sm:w-2/3 text-left mt-10 sm:mt-0",children:[(0,W.jsx)("p",{className:"font-general-regular text-primary-dark dark:text-primary-light text-2xl font-bold mb-7",children:i.ProjectInfo.ProjectDetailsHeading}),i.ProjectInfo.ProjectDetails.map((e=>(0,W.jsx)("p",{className:"font-general-regular mb-5 text-lg text-ternary-dark dark:text-ternary-light",children:e.details},e.id)))]})]})};var $=i(1605);const ee=()=>(0,W.jsx)($.P.div,{initial:{opacity:0},animate:{opacity:1,delay:1},transition:{ease:"easeInOut",duration:.6,delay:.15},className:"container mx-auto mt-5 sm:mt-10",children:(0,W.jsxs)(X,{children:[(0,W.jsx)(Y,{}),(0,W.jsx)(J,{}),(0,W.jsx)(Z,{})]})})},1117:(e,t,i)=>{e.exports=i.p+"static/media/ar-digital-twin-3.d26ce989db33ac4b8e05.png"},3418:(e,t,i)=>{e.exports=i.p+"static/media/pongal-1.33f5a0a9f9b7a6d30bb1.png"},537:(e,t,i)=>{e.exports=i.p+"static/media/vuplex-brower.cefa829e8cea479c7c6a.png"},5635:(e,t,i)=>{e.exports=i.p+"static/media/drone-3.4f31d62293faff61a1cb.png"},722:(e,t,i)=>{e.exports=i.p+"static/media/glove-gesture-2.8c48c5f5c64aebd14fcb.png"},2201:(e,t,i)=>{e.exports=i.p+"static/media/locomotion-1.03d6a630a81349c984a6.png"},9281:(e,t,i)=>{e.exports=i.p+"static/media/hull-cleaner-1.f2d41bd124473ad68311.png"},5613:(e,t,i)=>{e.exports=i.p+"static/media/itrollheimen-1.907f959af3ac900f07f0.png"},3765:(e,t,i)=>{e.exports=i.p+"static/media/kongsberg-1.eb22b34fc91e40b5b13e.jpeg"},546:(e,t,i)=>{e.exports=i.p+"static/media/magipaint-2.c7b90216bfdefefc4d48.png"},5537:(e,t,i)=>{e.exports=i.p+"static/media/stack-3.070eac608025f3b2e3e1.png"},3953:(e,t,i)=>{e.exports=i.p+"static/media/manomotion-internship-1.54fc7f3a131f117cbcc7.png"},254:(e,t,i)=>{e.exports=i.p+"static/media/manomotion-addon.0fe904e54e9abd14b436.png"},9978:(e,t,i)=>{e.exports=i.p+"static/media/salmon-1.a40124ef810e60c4d641.png"},2433:(e,t,i)=>{e.exports=i.p+"static/media/storybits.e637579b69f8a0e11d3a.png"},9697:(e,t,i)=>{e.exports=i.p+"static/media/tuio-client-package.d68475c18bae2370641b.png"},5944:(e,t,i)=>{e.exports=i.p+"static/media/vr-architecture-1.729e8a07e48af4c798f3.png"},5517:(e,t,i)=>{e.exports=i.p+"static/media/vr-dev-toolkit.c77b7e5776bb9a1a2c44.png"},3023:(e,t,i)=>{e.exports=i.p+"static/media/vr-hand-keypad-1.7175148afa053e48296d.png"},2308:(e,t,i)=>{e.exports=i.p+"static/media/vr-ship-1.68679bbabc116d2fa5be.png"},621:(e,t,i)=>{e.exports=i.p+"static/media/xr-presentations-1.141f783d0226af3fa570.png"}}]);
//# sourceMappingURL=580.03b08af8.chunk.js.map